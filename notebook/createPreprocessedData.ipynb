{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create preprocessed data for training MTL model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from similarity import Similarity\n",
    "from normalization import Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_density(df, top_k):\n",
    "    arr = df.values.T.copy() # canvert to numpy array\n",
    "    mask = arr != 0 # Convert all integer to boolean\n",
    "    congestion_density = mask.sum(axis=1) / mask.shape[1] # Calculate density\n",
    "    indecies = congestion_density.argsort()[::-1] # Sort by descending order\n",
    "    return indecies[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "1. Read files\n",
    "2. Get intersection of two time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehr_file = \"../data/congestionlength_for_hours.csv\"\n",
    "fivemin_file = \"../data/5MinFukushimaTrafficDataFrame.csv\"\n",
    "\n",
    "# Read data\n",
    "onehr_df = pd.read_csv(onehr_file)\n",
    "fivemin_df = pd.read_csv(fivemin_file)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "onehr_df[\"timestamp\"] = pd.to_datetime(onehr_df[\"timestamp\"])\n",
    "fivemin_df[\"timestamp\"] = pd.to_datetime(fivemin_df[\"timestamp\"])\n",
    "\n",
    "# Set timestamp as index\n",
    "onehr_df = onehr_df.set_index(\"timestamp\")\n",
    "fivemin_df = fivemin_df.set_index(\"timestamp\")\n",
    "\n",
    "# Get intersection of columns\n",
    "common_columns = fivemin_df.columns.intersection(onehr_df.columns)\n",
    "onehr_df = onehr_df[common_columns]\n",
    "fivemin_df = fivemin_df[common_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessed data (1hr)\n",
    "1. Calciulate the density of the time series of all columns\n",
    "2. Calculate the similarity of the time series of all columns\n",
    "3. Create preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10 # Top k nearest neighbors\n",
    "density_idx_1hr = calculate_density(onehr_df, top_k) # Get top k densest columns\n",
    "cos_sim = Similarity.cosine_similarity(onehr_df) # Calculate cosine similarity\n",
    "nearest_1hr = cos_sim.argsort(axis=1)[density_idx_1hr, ::-1][:top_k] # Get top k nearest neighbors\n",
    "new_df = pd.DataFrame(onehr_df.iloc[:, np.insert(nearest_1hr[0][0:top_k-1], 0, density_idx_1hr[0])]) # Create new dataframe with top k nearest neighbors\n",
    "new_df = Normalization.min_max_scaling(new_df) # Normalize data\n",
    "new_df.to_csv(\"../data/preprocessed1hrData.csv\") # Save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessed data (5min)\n",
    "1. Calciulate the density of the time series of all columns\n",
    "2. Calculate the similarity of the time series of all columns\n",
    "3. Create preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10 # Top k nearest neighbors\n",
    "density_idx_5min = calculate_density(fivemin_df, top_k) # Get top k densest columns\n",
    "cos_sim = Similarity.cosine_similarity(fivemin_df) # Calculate cosine similarity\n",
    "nearest_5min = cos_sim.argsort(axis=1)[density_idx_5min, ::-1][:top_k] # Get top k nearest neighbors\n",
    "new_df = pd.DataFrame(fivemin_df.iloc[:, np.insert(nearest_5min[0][0:top_k-1], 0, density_idx_5min[0])]) # Create new dataframe with top k nearest neighbors\n",
    "new_df = Normalization.min_max_scaling(new_df) # Normalize data\n",
    "new_df.to_csv(\"../data/preprocessed5minData.csv\") # Save to csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
